{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../res/tos.csv\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"Unnamed: 0\", axis=1)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>score</th>\n",
              "      <th>class</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>signal does not sell, rent or monetize your pe...</td>\n",
              "      <td>25</td>\n",
              "      <td>good</td>\n",
              "      <td>signal.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you must be at least 13 years old to use our s...</td>\n",
              "      <td>15</td>\n",
              "      <td>neutral</td>\n",
              "      <td>signal.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>you agree to resolve any claim you have with u...</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>signal.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we may modify, suspend, or terminate your acce...</td>\n",
              "      <td>60</td>\n",
              "      <td>bad</td>\n",
              "      <td>signal.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>we work with third parties to provide some of ...</td>\n",
              "      <td>15</td>\n",
              "      <td>bad</td>\n",
              "      <td>signal.json</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  score    class  \\\n",
              "0  signal does not sell, rent or monetize your pe...     25     good   \n",
              "1  you must be at least 13 years old to use our s...     15  neutral   \n",
              "2  you agree to resolve any claim you have with u...      0  neutral   \n",
              "3  we may modify, suspend, or terminate your acce...     60      bad   \n",
              "4  we work with third parties to provide some of ...     15      bad   \n",
              "\n",
              "      filename  \n",
              "0  signal.json  \n",
              "1  signal.json  \n",
              "2  signal.json  \n",
              "3  signal.json  \n",
              "4  signal.json  "
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import string"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(420)\n",
        "torch.cuda.is_available()\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_tokens(text):\n",
        "    if isinstance(text, str):\n",
        "#         print(f\"======={text}\")\n",
        "        text = text.split()\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        tokens = [w.translate(table) for w in text]\n",
        "        \n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens'] = df['data'].apply(prepare_tokens)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>score</th>\n",
              "      <th>class</th>\n",
              "      <th>filename</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>signal does not sell, rent or monetize your pe...</td>\n",
              "      <td>25</td>\n",
              "      <td>good</td>\n",
              "      <td>signal.json</td>\n",
              "      <td>signal does not sell rent or monetize your per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you must be at least 13 years old to use our s...</td>\n",
              "      <td>15</td>\n",
              "      <td>neutral</td>\n",
              "      <td>signal.json</td>\n",
              "      <td>you must be at least 13 years old to use our s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>you agree to resolve any claim you have with u...</td>\n",
              "      <td>0</td>\n",
              "      <td>neutral</td>\n",
              "      <td>signal.json</td>\n",
              "      <td>you agree to resolve any claim you have with u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we may modify, suspend, or terminate your acce...</td>\n",
              "      <td>60</td>\n",
              "      <td>bad</td>\n",
              "      <td>signal.json</td>\n",
              "      <td>we may modify suspend or terminate your access...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>we work with third parties to provide some of ...</td>\n",
              "      <td>15</td>\n",
              "      <td>bad</td>\n",
              "      <td>signal.json</td>\n",
              "      <td>we work with third parties to provide some of ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  score    class  \\\n",
              "0  signal does not sell, rent or monetize your pe...     25     good   \n",
              "1  you must be at least 13 years old to use our s...     15  neutral   \n",
              "2  you agree to resolve any claim you have with u...      0  neutral   \n",
              "3  we may modify, suspend, or terminate your acce...     60      bad   \n",
              "4  we work with third parties to provide some of ...     15      bad   \n",
              "\n",
              "      filename                                             tokens  \n",
              "0  signal.json  signal does not sell rent or monetize your per...  \n",
              "1  signal.json  you must be at least 13 years old to use our s...  \n",
              "2  signal.json  you agree to resolve any claim you have with u...  \n",
              "3  signal.json  we may modify suspend or terminate your access...  \n",
              "4  signal.json  we work with third parties to provide some of ...  "
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "df['class_id'] = 0\n",
        "df.loc[df[\"class\"]==\"neutral\", \"class_id\"] = 1\n",
        "df.loc[df[\"class\"]==\"good\", \"class_id\"] = 2\n",
        "df.loc[df[\"class\"]==\"blocker\", \"class_id\"] = 3"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.class_id.hist()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd8116a7dd8>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEiJJREFUeJzt3X+s3XV9x/HnWwqK3EkR9K5pu12MjRujc4ObWkdibq2RX4slGSQYIi3BNJlMcbBo5x8jc1mGfyATtmg6YZalsTA0awc4xwo3xmR0UkQKVkdlHVzoqFooXsG5bu/9cT7dbi633O895/Sce+7n+Uhu7vfH5/v9ft7fT+95ne/3/GhkJpKk+ryu3x2QJPWHASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmq1KJ+d+C1nHHGGTkyMtL29j/96U855ZRTutehPlkodYC1zEcLpQ6wlqN27979o8x8y2zt5nUAjIyM8PDDD7e9/fj4OGNjY93rUJ8slDrAWuajhVIHWMtREfHvTdp5C0iSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkio1rz8J3Kk9zx5mw6Z7e37c/Tde3PNjStJceQUgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVKzBkBE3B4RByPi8SnL3hwR90fEk+X3aWV5RMQtEbEvIh6LiHOmbLO+tH8yItYfn3IkSU01uQL4EnDBtGWbgJ2ZuQLYWeYBLgRWlJ+NwOehFRjADcC7gFXADUdDQ5LUH7MGQGZ+Azg0bfE6YEuZ3gJcMmX5HdnyELA4IpYA5wP3Z+ahzHwBuJ9Xh4okqYfafQ1gODMPAJTfby3LlwLPTGk3UZYda7kkqU+6/XXQMcOyfI3lr95BxEZat48YHh5mfHy87c4MnwzXrzzS9vbt6qTPM5mcnOz6PvvFWuafhVIHWMtctRsAz0fEksw8UG7xHCzLJ4DlU9otA54ry8emLR+faceZuRnYDDA6OppjY2MzNWvk1q3buWlP7//Lg/1XjHV1f+Pj43RyHuYTa5l/FkodYC1z1e4toB3A0XfyrAe2T1l+ZXk30GrgcLlF9HXg/RFxWnnx9/1lmSSpT2Z9ehwRX6b17P2MiJig9W6eG4G7IuJq4GngstL8PuAiYB/wMnAVQGYeiog/Ab5V2n06M6e/sCxJ6qFZAyAzP3iMVWtnaJvANcfYz+3A7XPqnSTpuPGTwJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSHQVARPx+RDwREY9HxJcj4g0RcWZE7IqIJyPizog4qbR9fZnfV9aPdKMASVJ72g6AiFgKfAwYzcyzgROAy4HPADdn5grgBeDqssnVwAuZ+Xbg5tJOktQnnd4CWgScHBGLgDcCB4D3AneX9VuAS8r0ujJPWb82IqLD40uS2hSZ2f7GEdcCfwq8AvwjcC3wUHmWT0QsB76WmWdHxOPABZk5Udb9AHhXZv5o2j43AhsBhoeHz922bVvb/Tt46DDPv9L25m1bufTUru5vcnKSoaGhru6zX6xl/lkodYC1HLVmzZrdmTk6W7tFbe0diIjTaD2rPxN4Efhb4MIZmh5NmJme7b8qfTJzM7AZYHR0NMfGxtrtIrdu3c5Ne9ousW37rxjr6v7Gx8fp5DzMJ9Yy/yyUOsBa5qqTW0DvA/4tM3+Ymf8FfBX4LWBxuSUEsAx4rkxPAMsByvpTgUMdHF+S1IFOAuBpYHVEvLHcy18LfBd4ELi0tFkPbC/TO8o8Zf0D2cn9J0lSR9oOgMzcRevF3EeAPWVfm4FPAtdFxD7gdOC2ssltwOll+XXApg76LUnqUEc3yDPzBuCGaYufAlbN0PZnwGWdHE+S1D1+EliSKmUASFKlDABJqpQBIEmV6v2npLQgjWy6t1G761ceYUPDtk3sv/Hiru1Lqo1XAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEp1FAARsTgi7o6I70XE3oh4d0S8OSLuj4gny+/TStuIiFsiYl9EPBYR53SnBElSOzq9Avgc8A+Z+SvAO4G9wCZgZ2auAHaWeYALgRXlZyPw+Q6PLUnqQNsBEBFvAt4D3AaQmT/PzBeBdcCW0mwLcEmZXgfckS0PAYsjYknbPZckdaSTK4C3AT8E/joivh0RX4yIU4DhzDwAUH6/tbRfCjwzZfuJskyS1AeRme1tGDEKPAScl5m7IuJzwEvARzNz8ZR2L2TmaRFxL/BnmfnNsnwn8InM3D1tvxtp3SJieHj43G3btrXVP4CDhw7z/Cttb962lUtP7er+JicnGRoa6uo+u23Ps4cbtRs+ma6OSbfP9VwMwrg0sVDqAGs5as2aNbszc3S2dova2nvLBDCRmbvK/N207vc/HxFLMvNAucVzcEr75VO2XwY8N32nmbkZ2AwwOjqaY2NjbXfw1q3buWlPJyW2Z/8VY13d3/j4OJ2ch17YsOneRu2uX3mkq2PS7XM9F4MwLk0slDrAWuaq7VtAmfkfwDMR8Y6yaC3wXWAHsL4sWw9sL9M7gCvLu4FWA4eP3iqSJPVep0/FPgpsjYiTgKeAq2iFyl0RcTXwNHBZaXsfcBGwD3i5tJUk9UlHAZCZjwIz3WdaO0PbBK7p5HiSpO7xk8CSVKnev0IqqSMjDV9wb+r6lUcav4i//8aLu3ps9ZdXAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpToOgIg4ISK+HRH3lPkzI2JXRDwZEXdGxEll+evL/L6yfqTTY0uS2teNK4Brgb1T5j8D3JyZK4AXgKvL8quBFzLz7cDNpZ0kqU86CoCIWAZcDHyxzAfwXuDu0mQLcEmZXlfmKevXlvaSpD7o9Argz4FPAP9T5k8HXszMI2V+AlhappcCzwCU9YdLe0lSH0RmtrdhxG8DF2XmRyJiDPgD4Crgn8ttHiJiOXBfZq6MiCeA8zNzoqz7AbAqM388bb8bgY0Aw8PD527btq29yoCDhw7z/Cttb962lUtP7er+JicnGRoa6uo+u23Ps4cbtRs+ma6OSbfP9Vz0a1yanuum5jIm/TzfTQzC30pTndSyZs2a3Zk5Olu7RW3tveU84AMRcRHwBuBNtK4IFkfEovIsfxnwXGk/ASwHJiJiEXAqcGj6TjNzM7AZYHR0NMfGxtru4K1bt3PTnk5KbM/+K8a6ur/x8XE6OQ+9sGHTvY3aXb/ySFfHpNvnei76NS5Nz3VTcxmTfp7vJgbhb6WpXtTS9i2gzPzDzFyWmSPA5cADmXkF8CBwaWm2HthepneUecr6B7Ldyw9JUseOx+cAPglcFxH7aN3jv60svw04vSy/Dth0HI4tSWqoK9fimTkOjJfpp4BVM7T5GXBZN44nSeqcnwSWpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlWo7ACJieUQ8GBF7I+KJiLi2LH9zRNwfEU+W36eV5RERt0TEvoh4LCLO6VYRkqS56+QK4AhwfWb+KrAauCYizgI2ATszcwWws8wDXAisKD8bgc93cGxJUofaDoDMPJCZj5TpnwB7gaXAOmBLabYFuKRMrwPuyJaHgMURsaTtnkuSOhKZ2flOIkaAbwBnA09n5uIp617IzNMi4h7gxsz8Zlm+E/hkZj48bV8baV0hMDw8fO62bdva7tfBQ4d5/pW2N2/byqWndnV/k5OTDA0NdXWf3bbn2cON2g2fTFfHpNvnei76NS5Nz3VTcxmTfp7vJgbhb6WpTmpZs2bN7swcna3dorb2PkVEDAFfAT6emS9FxDGbzrDsVemTmZuBzQCjo6M5NjbWdt9u3bqdm/Z0XOKc7b9irKv7Gx8fp5Pz0AsbNt3bqN31K490dUy6fa7nol/j0vRcNzWXMenn+W5iEP5WmupFLR29CygiTqT14L81M79aFj9/9NZO+X2wLJ8Alk/ZfBnwXCfHlyS1r5N3AQVwG7A3Mz87ZdUOYH2ZXg9sn7L8yvJuoNXA4cw80O7xJUmd6eRa/DzgQ8CeiHi0LPsUcCNwV0RcDTwNXFbW3QdcBOwDXgau6uDYkqQOtR0A5cXcY93wXztD+wSuafd4kqTu8pPAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFVqUb87IEnz1cime/t27C9dcMpxP4ZXAJJUKQNAkiplAEhSpQwASaqUASBJlep5AETEBRHx/YjYFxGben18SVJLTwMgIk4A/hK4EDgL+GBEnNXLPkiSWnp9BbAK2JeZT2Xmz4FtwLoe90GSRO8DYCnwzJT5ibJMktRjkZm9O1jEZcD5mfnhMv8hYFVmfnRKm43AxjL7DuD7HRzyDOBHHWw/XyyUOsBa5qOFUgdYy1G/nJlvma1Rr78KYgJYPmV+GfDc1AaZuRnY3I2DRcTDmTnajX3100KpA6xlPloodYC1zFWvbwF9C1gREWdGxEnA5cCOHvdBkkSPrwAy80hE/B7wdeAE4PbMfKKXfZAktfT820Az8z7gvh4driu3kuaBhVIHWMt8tFDqAGuZk56+CCxJmj/8KghJqtTAB8BsXy0REa+PiDvL+l0RMdL7XjbToJYNEfHDiHi0/Hy4H/2cTUTcHhEHI+LxY6yPiLil1PlYRJzT6z421aCWsYg4PGVM/qjXfWwiIpZHxIMRsTcinoiIa2doMxDj0rCWQRmXN0TEv0TEd0otfzxDm+P3GJaZA/tD64XkHwBvA04CvgOcNa3NR4AvlOnLgTv73e8OatkA/EW/+9qglvcA5wCPH2P9RcDXgABWA7v63ecOahkD7ul3PxvUsQQ4p0z/AvCvM/z7GohxaVjLoIxLAENl+kRgF7B6Wpvj9hg26FcATb5aYh2wpUzfDayNiOhhH5taMF+TkZnfAA69RpN1wB3Z8hCwOCKW9KZ3c9OgloGQmQcy85Ey/RNgL6/+FP5AjEvDWgZCOdeTZfbE8jP9hdnj9hg26AHQ5Ksl/q9NZh4BDgOn96R3c9P0azJ+p1ye3x0Ry2dYPwgW2leCvLtcwn8tIn6t352ZTbmF8Ju0nm1ONXDj8hq1wICMS0ScEBGPAgeB+zPzmOPS7cewQQ+AmVJweno2aTMfNOnn3wMjmfnrwD/x/88KBs2gjEkTj9D62P07gVuBv+tzf15TRAwBXwE+npkvTV89wybzdlxmqWVgxiUz/zszf4PWNyOsioizpzU5buMy6AEw61dLTG0TEYuAU5mfl/RNvibjx5n5n2X2r4Bze9S3bmsybgMhM186egmfrc+4nBgRZ/S5WzOKiBNpPWBuzcyvztBkYMZltloGaVyOyswXgXHggmmrjttj2KAHQJOvltgBrC/TlwIPZHk1ZZ6ZtZZp92M/QOve5yDaAVxZ3nWyGjicmQf63al2RMQvHr0fGxGraP1N/bi/vXq10sfbgL2Z+dljNBuIcWlSywCNy1siYnGZPhl4H/C9ac2O22NYzz8J3E15jK+WiIhPAw9n5g5a/1D+JiL20UrNy/vX42NrWMvHIuIDwBFatWzoW4dfQ0R8mda7MM6IiAngBlovbpGZX6D1SfCLgH3Ay8BV/enp7BrUcinwuxFxBHgFuHyePsE4D/gQsKfcbwb4FPBLMHDj0qSWQRmXJcCWaP1nWa8D7srMe3r1GOYngSWpUoN+C0iS1CYDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkSv0vnj+jSHH1QqwAAAAASUVORK5CYII=\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[df.class_id != 3]"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": [
              "(2397, 6)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "word_to_ix = {'PAD': 0}\n",
        "for sent in df.tokens.tolist():\n",
        "    if sent:\n",
        "#         print(sent)\n",
        "        for word in sent.split():\n",
        "            if word not in word_to_ix:\n",
        "                word_to_ix[word] = len(word_to_ix)\n",
        "# print(word_to_ix)\n",
        "tag_to_ix = {\"bad\": 0, \"neutral\": 1, \"good\": 2, \"blocker\": 3}"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_to_ix)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": [
              "6250"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "BATCH_SIZE = 16"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class TOSClaasifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size, batch_size):\n",
        "        super(TOSClaasifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.target_size = target_size\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=0.3, num_layers=2)\n",
        "        \n",
        "        self.h1 = nn.Linear(hidden_dim*150, 512)\n",
        "        self.hidden2tag = nn.Linear(512, target_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.embed_dim = embedding_dim\n",
        "        \n",
        "\n",
        "    def forward(self, sentence):\n",
        "#         print(sentence.shape)\n",
        "        embeds = self.word_embeddings(sentence.view(self.batch_size,-1))\n",
        "#         print(embeds.shape)\n",
        "        lstm_out, _ = self.LSTM(embeds.view(self.batch_size, 150, self.embed_dim))\n",
        "#         print(lstm_out.shape)\n",
        "        lstm_out = lstm_out.contiguous().view(self.batch_size,-1)\n",
        "#         print(lstm_out.shape)\n",
        "        tag_space = self.h1(lstm_out)\n",
        "#         print(tag_space.shape)\n",
        "\n",
        "        tag_space = self.hidden2tag(tag_space)\n",
        "#         print(tag_space.shape)\n",
        "        \n",
        "#         tag_score = F.log_softmax(tag_space.view(self.batch_size, -1, self.target_size))\n",
        "        tag_score = F.log_softmax(tag_space, dim=0)\n",
        "        tag_score = tag_score.view(self.batch_size, self.target_size)\n",
        "#         print(tag_score)\n",
        "#         tag_score = tag_score[:, -1] # get last batch of labels\n",
        "        return tag_score\n",
        "        "
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = TOSClaasifier(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), 3, BATCH_SIZE)\n",
        "model = model.cuda()\n"
      ],
      "outputs": [],
      "execution_count": 55,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss().cuda()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "# optimizer = optim.RMSprop(model.parameters())\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq.split()]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOSClaasifier(\n",
            "  (word_embeddings): Embedding(6250, 128)\n",
            "  (LSTM): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
            "  (h1): Linear(in_features=38400, out_features=512, bias=True)\n",
            "  (hidden2tag): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "execution_count": 58,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[~df.tokens.isna()]"
      ],
      "outputs": [],
      "execution_count": 59,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            zeroes = np.zeros(seq_length-review_len).astype(np.int)\n",
        "#             print(zeroes)\n",
        "#             print(review.numpy())\n",
        "            new = np.concatenate((review.numpy(),zeroes), axis=None)\n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "outputs": [],
      "execution_count": 60,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "seq_ids = df.tokens.apply(prepare_sequence, args=[word_to_ix]).tolist()\n",
        "label_ids = df['class'].apply(prepare_sequence, args=[tag_to_ix]).tolist()"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feats = pad_features(seq_ids, 150)"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feats"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": [
              "array([[   1,    2,    3, ...,    0,    0,    0],\n",
              "       [  17,   18,   19, ...,    0,    0,    0],\n",
              "       [  17,   39,   25, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  87,  185,    3, ...,    0,    0,    0],\n",
              "       [ 122, 2832,   25, ...,    0,    0,    0],\n",
              "       [  87,  158,   17, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 63,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = feats[0:int(0.75*len(feats))]\n",
        "test_X = feats[int(0.75*len(feats)):]\n",
        "\n",
        "train_y = label_ids[0:int(0.75*len(label_ids))]\n",
        "test_y = label_ids[int(0.75*len(label_ids)):]"
      ],
      "outputs": [],
      "execution_count": 65,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 66,
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  6, 11, 12, 13, 14, 15, 16,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 66,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[0]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": [
              "tensor([2])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 67,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def next_batch(X,y):\n",
        "    i = 0\n",
        "    n = len(X)\n",
        "    \n",
        "    while i <= n:\n",
        "        yield X[i:i+BATCH_SIZE], y[i:i+BATCH_SIZE]\n",
        "        i = i + BATCH_SIZE"
      ],
      "outputs": [],
      "execution_count": 68,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in next_batch([0,1,2,3,4,5,6], [0,1,2,1,1,1,1]):\n",
        "    print(x,y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6] [0, 1, 2, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "execution_count": 69,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.cuda()\n",
        "\n",
        "for epoch in range(100):\n",
        "    print(f\"==================================={epoch}================================\")\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    train_idx = 0\n",
        "\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    test_idx = 0\n",
        "\n",
        "#     scheduler.step(epoch)\n",
        "#     for sentence, label in list(zip(train_X, train_y)):\n",
        "    for sentence, label in next_batch(train_X, train_y):\n",
        "        if len(sentence) < BATCH_SIZE:\n",
        "            continue\n",
        "#         print(sentence, label)\n",
        "        model.zero_grad()\n",
        "\n",
        "        tag_scores = model(torch.LongTensor(sentence).view(BATCH_SIZE,150).cuda())\n",
        "        loss = loss_fn(tag_scores, torch.LongTensor(label).cuda())\n",
        "#         print(loss)\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += (tag_scores.argmax(1) == torch.LongTensor(label).cuda()).sum().item()\n",
        "        train_idx += BATCH_SIZE\n",
        "        \n",
        "    print(train_loss / train_idx)\n",
        "    print(train_acc / train_idx)\n",
        "        \n",
        "    \n",
        "    for sentence, label in next_batch(test_X, test_y):\n",
        "        if len(sentence) < BATCH_SIZE:\n",
        "            continue\n",
        "#         text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            tag_scores = model(torch.LongTensor(sentence).view(BATCH_SIZE,150).cuda())\n",
        "\n",
        "            loss = loss_fn(tag_scores.view(BATCH_SIZE,-1), torch.LongTensor(label).cuda())\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "            test_acc += (tag_scores.argmax(1) == torch.LongTensor(label).cuda()).sum().item()\n",
        "            test_idx += BATCH_SIZE\n",
        "\n",
        "    print(test_loss / test_idx)\n",
        "    print(test_acc/ test_idx)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================0================================\n",
            "0.1676425566630704\n",
            "0.5323660714285714\n",
            "0.16312180902506854\n",
            "0.5641891891891891\n",
            "===================================1================================\n",
            "0.15320122654416732\n",
            "0.7109375\n",
            "0.16254338420726158\n",
            "0.5591216216216216\n",
            "===================================2================================\n",
            "0.14506222680211067\n",
            "0.8108258928571429\n",
            "0.16943554419118004\n",
            "0.5625\n",
            "===================================3================================\n",
            "0.14281098172068596\n",
            "0.8376116071428571\n",
            "0.1657368129975087\n",
            "0.5912162162162162\n",
            "===================================4================================\n",
            "0.14040539945874894\n",
            "0.8677455357142857\n",
            "0.1674092911385201\n",
            "0.6081081081081081\n",
            "===================================5================================\n",
            "0.13423863339370914\n",
            "0.91796875\n",
            "0.16582429570120735\n",
            "0.6047297297297297\n",
            "===================================6================================\n",
            "0.13146132357152446\n",
            "0.9386160714285714\n",
            "0.16294508568338445\n",
            "0.6317567567567568\n",
            "===================================7================================\n",
            "0.1285217214109642\n",
            "0.95703125\n",
            "0.1630300247185939\n",
            "0.6554054054054054\n",
            "===================================8================================\n",
            "0.1274209332519344\n",
            "0.9665178571428571\n",
            "0.16050105924541885\n",
            "0.6554054054054054\n",
            "===================================9================================\n",
            "0.12618207432595746\n",
            "0.9732142857142857\n",
            "0.1599084622151143\n",
            "0.6469594594594594\n",
            "===================================10================================\n",
            "0.12567483341055258\n",
            "0.9676339285714286\n",
            "0.16261130611638766\n",
            "0.643581081081081\n",
            "===================================11================================\n",
            "0.12486639991402626\n",
            "0.98046875\n",
            "0.16223701150030703\n",
            "0.6300675675675675\n",
            "===================================12================================\n",
            "0.12338130655033248\n",
            "0.9849330357142857\n",
            "0.16826144787105354\n",
            "0.660472972972973\n",
            "===================================13================================\n",
            "0.12280319883887257\n",
            "0.9854910714285714\n",
            "0.16447896651319555\n",
            "0.6503378378378378\n",
            "===================================14================================\n",
            "0.12264802074059844\n",
            "0.984375\n",
            "0.16385203237469131\n",
            "0.6452702702702703\n",
            "===================================15================================\n",
            "0.12128334964758583\n",
            "0.9888392857142857\n",
            "0.16278329572162112\n",
            "0.660472972972973\n",
            "===================================16================================\n",
            "0.1213826687474336\n",
            "0.9860491071428571\n",
            "0.16366078483091817\n",
            "0.6706081081081081\n",
            "===================================17================================\n",
            "0.12123653611966542\n",
            "0.9877232142857143\n",
            "0.15996750464310516\n",
            "0.660472972972973\n",
            "===================================18================================\n",
            "0.12078173604926892\n",
            "0.9910714285714286\n",
            "0.1654334152872498\n",
            "0.6503378378378378\n",
            "===================================19================================\n",
            "0.12042281444051436\n",
            "0.9916294642857143\n",
            "0.1632524418669778\n",
            "0.6739864864864865\n",
            "===================================20================================\n",
            "0.12030223231496555\n",
            "0.9871651785714286\n",
            "0.16232833991179596\n",
            "0.6875\n",
            "===================================21================================\n",
            "0.11928188228713614\n",
            "0.9927455357142857\n",
            "0.16509639250265584\n",
            "0.6824324324324325\n",
            "===================================22================================\n",
            "0.11952473702175277\n",
            "0.9910714285714286\n",
            "0.16204611352972081\n",
            "0.7077702702702703\n",
            "===================================23================================\n",
            "0.11983865080401301\n",
            "0.9899553571428571\n",
            "0.15936736037602295\n",
            "0.6807432432432432\n",
            "===================================24================================\n",
            "0.1192127999051341\n",
            "0.9910714285714286\n",
            "0.16058668615044774\n",
            "0.6959459459459459\n",
            "===================================25================================\n",
            "0.11857332162825125\n",
            "0.9916294642857143\n",
            "0.16305091735478994\n",
            "0.6942567567567568\n",
            "===================================26================================\n",
            "0.11867253316034164\n",
            "0.9893973214285714\n",
            "0.16128385066986084\n",
            "0.6993243243243243\n",
            "===================================27================================\n",
            "0.1182322089693376\n",
            "0.9921875\n",
            "0.16177536345816948\n",
            "0.7010135135135135\n",
            "===================================28================================\n",
            "0.1177797992048519\n",
            "0.9938616071428571\n",
            "0.16543584860659935\n",
            "0.706081081081081\n",
            "===================================29================================\n",
            "0.11758173789296832\n",
            "0.9905133928571429\n",
            "0.1617424343083356\n",
            "0.7179054054054054\n",
            "===================================30================================\n",
            "0.11764855263754725\n",
            "0.9893973214285714\n",
            "0.16099446125932643\n",
            "0.7010135135135135\n",
            "===================================31================================\n",
            "0.11705915862694383\n",
            "0.9905133928571429\n",
            "0.16326406598091125\n",
            "0.7077702702702703\n",
            "===================================32================================\n",
            "0.11688852995367986\n",
            "0.9921875\n",
            "0.16832795457260027\n",
            "0.6959459459459459\n",
            "===================================33================================\n",
            "0.11687598490555372\n",
            "0.9893973214285714\n",
            "0.16817595749287992\n",
            "0.6891891891891891\n",
            "===================================34================================\n",
            "0.1167238846953426\n",
            "0.9905133928571429\n",
            "0.16897942085523862\n",
            "0.6891891891891891\n",
            "===================================35================================\n",
            "0.11645553919619747\n",
            "0.9893973214285714\n",
            "0.16867469814983574\n",
            "0.706081081081081\n",
            "===================================36================================\n",
            "0.11619683441572956\n",
            "0.9893973214285714\n",
            "0.17734147809647224\n",
            "0.7010135135135135\n",
            "===================================37================================\n",
            "0.11580104986205697\n",
            "0.9921875\n",
            "0.18535904384948113\n",
            "0.7010135135135135\n",
            "===================================38================================\n",
            "0.11546562578795212\n",
            "0.9927455357142857\n",
            "0.18604567868484034\n",
            "0.6739864864864865\n",
            "===================================39================================\n",
            "0.11684631656057068\n",
            "0.9866071428571429\n",
            "0.16734354681259878\n",
            "0.7010135135135135\n",
            "===================================40================================\n",
            "0.11645883873903326\n",
            "0.9877232142857143\n",
            "0.1723349537398364\n",
            "0.7212837837837838\n",
            "===================================41================================\n",
            "0.11545278644189239\n",
            "0.9910714285714286\n",
            "0.18129562405315605\n",
            "0.7077702702702703\n",
            "===================================42================================\n",
            "0.11522981491205948\n",
            "0.9905133928571429\n",
            "0.17711907907112226\n",
            "0.6773648648648649\n",
            "===================================43================================\n",
            "0.11493771676240223\n",
            "0.9938616071428571\n",
            "0.22272893383696274\n",
            "0.6402027027027027\n",
            "===================================44================================\n",
            "0.11459117123324956\n",
            "0.9916294642857143\n",
            "0.21596556740838127\n",
            "0.6739864864864865\n",
            "===================================58================================\n",
            "0.11627962992393545\n",
            "0.9854910714285714\n",
            "0.1898788491616378\n",
            "0.706081081081081\n",
            "===================================59================================\n",
            "0.11489556430439864\n",
            "0.9899553571428571\n",
            "0.1938371460985493\n",
            "0.7331081081081081\n",
            "===================================60================================\n",
            "0.11435032961890101\n",
            "0.9944196428571429\n",
            "0.20707764536947817\n",
            "0.7280405405405406\n",
            "===================================61================================\n",
            "0.1141777868781771\n",
            "0.9949776785714286\n",
            "0.207656545413507\n",
            "0.7246621621621622\n",
            "===================================62================================\n",
            "0.11412357884858336\n",
            "0.9944196428571429\n",
            "0.2149545336897309\n",
            "0.7280405405405406\n",
            "===================================63================================\n",
            "0.1140856158121356\n",
            "0.9938616071428571\n",
            "0.21820753611422875\n",
            "0.7246621621621622\n",
            "===================================64================================\n",
            "0.11407013217519436\n",
            "0.9949776785714286\n",
            "0.2214567786132967\n",
            "0.7297297297297297\n",
            "===================================65================================\n",
            "0.11405715911782213\n",
            "0.9944196428571429\n",
            "0.21955793209978053\n",
            "0.7246621621621622\n",
            "===================================66================================\n",
            "0.11405747536835927\n",
            "0.9949776785714286\n",
            "0.22437964903341756\n",
            "0.7246621621621622\n",
            "===================================67================================\n",
            "0.11405947059392929\n",
            "0.9927455357142857\n",
            "0.22442638672686913\n",
            "0.7297297297297297\n",
            "===================================68================================\n",
            "0.1174276244959661\n",
            "0.9787946428571429\n",
            "0.1700906076946774\n",
            "0.7010135135135135\n",
            "===================================69================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11637380280132804\n",
            "0.9888392857142857\n",
            "0.1824006740305875\n",
            "0.7212837837837838\n",
            "===================================70================================\n",
            "0.11499134158449513\n",
            "0.9927455357142857\n",
            "0.19332612527383342\n",
            "0.6993243243243243\n",
            "===================================71================================\n",
            "0.11448410046952111\n",
            "0.9933035714285714\n",
            "0.198714489066923\n",
            "0.7128378378378378\n",
            "===================================72================================\n",
            "0.11418942428593125\n",
            "0.9938616071428571\n",
            "0.20650979473784165\n",
            "0.714527027027027\n",
            "===================================73================================\n",
            "0.1141357028058597\n",
            "0.9949776785714286\n",
            "0.20303312910569682\n",
            "0.7212837837837838\n",
            "===================================74================================\n",
            "0.1140963852272502\n",
            "0.9938616071428571\n",
            "0.20948010078958562\n",
            "0.7212837837837838\n",
            "===================================75================================\n",
            "0.11410007951781154\n",
            "0.9944196428571429\n",
            "0.21076922722764918\n",
            "0.722972972972973\n",
            "===================================76================================\n",
            "0.11408814994086112\n",
            "0.9933035714285714\n",
            "0.2123973244750822\n",
            "0.7212837837837838\n",
            "===================================77================================\n",
            "0.11408514136980687\n",
            "0.9933035714285714\n",
            "0.21474465244525187\n",
            "0.7195945945945946\n",
            "===================================78================================\n",
            "0.11405754847718137\n",
            "0.9944196428571429\n",
            "0.21497046222557892\n",
            "0.7212837837837838\n",
            "===================================79================================\n",
            "0.11404194909014873\n",
            "0.9949776785714286\n",
            "0.21894645127090248\n",
            "0.7195945945945946\n",
            "===================================80================================\n",
            "0.11404820225600686\n",
            "0.9944196428571429\n",
            "0.22012467360174334\n",
            "0.7195945945945946\n",
            "===================================81================================\n",
            "0.11404345676835094\n",
            "0.9933035714285714\n",
            "0.22103687677834485\n",
            "0.714527027027027\n",
            "===================================82================================\n",
            "0.11404379417321511\n",
            "0.9933035714285714\n",
            "0.21780534692712733\n",
            "0.7212837837837838\n",
            "===================================83================================\n",
            "0.1140433166708265\n",
            "0.9949776785714286\n",
            "0.22535893804318197\n",
            "0.7162162162162162\n",
            "===================================84================================\n",
            "0.11404598870181612\n",
            "0.9949776785714286\n",
            "0.22100393031094526\n",
            "0.7212837837837838\n",
            "===================================85================================\n",
            "0.11403155320190958\n",
            "0.9949776785714286\n",
            "0.226718909031636\n",
            "0.722972972972973\n",
            "===================================86================================\n",
            "0.11402716780347484\n",
            "0.9944196428571429\n",
            "0.22424655427803863\n",
            "0.7179054054054054\n",
            "===================================87================================\n",
            "0.11403360424031105\n",
            "0.9938616071428571\n",
            "0.2279893216249105\n",
            "0.7195945945945946\n",
            "===================================88================================\n",
            "0.11403568641149572\n",
            "0.9933035714285714\n",
            "0.2225601431485769\n",
            "0.7246621621621622\n",
            "===================================89================================\n",
            "0.11404450410710913\n",
            "0.9938616071428571\n",
            "0.2346832470313923\n",
            "0.7212837837837838\n",
            "===================================90================================\n",
            "0.11404693612296667\n",
            "0.9938616071428571\n",
            "0.22484621325054685\n",
            "0.7263513513513513\n",
            "===================================91================================\n",
            "0.11405228929860252\n",
            "0.9944196428571429\n",
            "0.22426385855352557\n",
            "0.7195945945945946\n",
            "===================================92================================\n",
            "0.1140402052551508\n",
            "0.9944196428571429\n",
            "0.2409827531995\n",
            "0.722972972972973\n",
            "===================================93================================\n",
            "0.1140773030264037\n",
            "0.9938616071428571\n",
            "0.23400773389919385\n",
            "0.722972972972973\n",
            "===================================94================================\n",
            "0.11407798315797534\n",
            "0.9927455357142857\n",
            "0.23276010477865064\n",
            "0.7162162162162162\n",
            "===================================95================================\n",
            "0.11405563194836889\n",
            "0.9949776785714286\n",
            "0.22501449729945208\n",
            "0.722972972972973\n",
            "===================================96================================\n",
            "0.11403617967984506\n",
            "0.9938616071428571\n",
            "0.22964333964360728\n",
            "0.7162162162162162\n",
            "===================================97================================\n",
            "0.11403595549719674\n",
            "0.9949776785714286\n",
            "0.23471958935260773\n",
            "0.7128378378378378\n",
            "===================================98================================\n",
            "0.11402986737500344\n",
            "0.9933035714285714\n",
            "0.24567982272521868\n",
            "0.7195945945945946\n",
            "===================================99================================\n",
            "0.11401955736801028\n",
            "0.9938616071428571\n",
            "0.24226742580130295\n",
            "0.7111486486486487\n"
          ]
        }
      ],
      "execution_count": 70,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence, label in next_batch(test_X, test_y):\n",
        "        if len(sentence) < BATCH_SIZE:\n",
        "            continue\n",
        "#         text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad():\n",
        "            tag_scores = model(torch.LongTensor(sentence).view(BATCH_SIZE,150).cuda())\n",
        "            print(\"***********\")\n",
        "#             print(tag_scores.shape)\n",
        "#             print(tag_scores.view(BATCH_SIZE,-1).shape)\n",
        "            loss = loss_fn(tag_scores, torch.LongTensor(label).cuda())\n",
        "            test_loss += loss.item()\n",
        "            \n",
        "#             print(tag_scores.shape)\n",
        "            print(tag_scores.argmax(1))\n",
        "            print(label)\n",
        "            \n",
        "            test_acc += (tag_scores.argmax(1) == torch.LongTensor(label).cuda()).sum().item()\n",
        "            test_idx += BATCH_SIZE\n",
        "            \n",
        "            print(test_acc/ test_idx)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***********\n",
            "tensor([2, 2, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([1]), tensor([2]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([2])]\n",
            "0.7125\n",
            "***********\n",
            "tensor([0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([0])]\n",
            "0.7154605263157895\n",
            "***********\n",
            "tensor([1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 1, 2], device='cuda:0')\n",
            "[tensor([1]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2])]\n",
            "0.7175324675324676\n",
            "***********\n",
            "tensor([2, 1, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2], device='cuda:0')\n",
            "[tensor([1]), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([1])]\n",
            "0.7163461538461539\n",
            "***********\n",
            "tensor([1, 1, 1, 1, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 0, 1], device='cuda:0')\n",
            "[tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([0]), tensor([1])]\n",
            "0.7175632911392406\n",
            "***********\n",
            "tensor([0, 1, 2, 2, 2, 0, 0, 0, 2, 0, 2, 1, 0, 1, 2, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([2]), tensor([2]), tensor([0])]\n",
            "0.71796875\n",
            "***********\n",
            "tensor([2, 2, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1])]\n",
            "0.7160493827160493\n",
            "***********\n",
            "tensor([1, 1, 1, 1, 1, 0, 2, 2, 0, 0, 2, 0, 1, 2, 0, 1], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])]\n",
            "0.7126524390243902\n",
            "***********\n",
            "tensor([2, 2, 2, 2, 2, 1, 2, 0, 0, 1, 0, 2, 1, 0, 1, 0], device='cuda:0')\n",
            "[tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([0])]\n",
            "0.7093373493975904\n",
            "***********\n",
            "tensor([2, 2, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([1]), tensor([2])]\n",
            "0.7098214285714286\n",
            "***********\n",
            "tensor([2, 1, 2, 2, 2, 1, 1, 1, 2, 0, 2, 1, 2, 2, 2, 2], device='cuda:0')\n",
            "[tensor([2]), tensor([1]), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([2]), tensor([2]), tensor([2])]\n",
            "0.7088235294117647\n",
            "***********\n",
            "tensor([0, 0, 2, 1, 2, 0, 2, 1, 2, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1])]\n",
            "0.7107558139534884\n",
            "***********\n",
            "tensor([1, 2, 1, 2, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 2], device='cuda:0')\n",
            "[tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([2])]\n",
            "0.7104885057471264\n",
            "***********\n",
            "tensor([2, 0, 1, 2, 0, 0, 2, 1, 2, 2, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([0]), tensor([2]), tensor([1]), tensor([2]), tensor([2]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0])]\n",
            "0.7116477272727273\n",
            "***********\n",
            "tensor([2, 2, 2, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 0], device='cuda:0')\n",
            "[tensor([1]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([2]), tensor([0])]\n",
            "0.7134831460674157\n",
            "***********\n",
            "tensor([0, 0, 1, 1, 2, 2, 2, 1, 2, 1, 2, 0, 0, 0, 2, 2], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0])]\n",
            "0.7138888888888889\n",
            "***********\n",
            "tensor([1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 2, 2, 0, 2, 2, 1], device='cuda:0')\n",
            "[tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([1]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0])]\n",
            "0.7156593406593407\n",
            "***********\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([2]), tensor([0])]\n",
            "0.717391304347826\n",
            "***********\n",
            "tensor([0, 0, 0, 1, 1, 1, 2, 1, 2, 0, 2, 2, 0, 0, 1, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([2]), tensor([1]), tensor([1]), tensor([0]), tensor([2]), tensor([1]), tensor([0]), tensor([0]), tensor([2]), tensor([0])]\n",
            "0.717741935483871\n",
            "***********\n",
            "tensor([0, 2, 2, 1, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 1, 2], device='cuda:0')\n",
            "[tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([1]), tensor([1])]\n",
            "0.7160904255319149\n",
            "***********\n",
            "tensor([0, 0, 2, 1, 1, 2, 1, 0, 2, 2, 2, 0, 0, 1, 2, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1])]\n",
            "0.7144736842105263\n",
            "***********\n",
            "tensor([2, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([0])]\n",
            "0.7141927083333334\n",
            "***********\n",
            "tensor([2, 0, 2, 2, 0, 2, 2, 1, 2, 0, 0, 2, 0, 1, 1, 1], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([2]), tensor([1]), tensor([2]), tensor([1]), tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([1])]\n",
            "0.7139175257731959\n",
            "***********\n",
            "tensor([0, 2, 2, 0, 1, 1, 1, 2, 0, 0, 0, 0, 2, 1, 1, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([0])]\n",
            "0.7136479591836735\n",
            "***********\n",
            "tensor([2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])]\n",
            "0.7127525252525253\n",
            "***********\n",
            "tensor([0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 0, 1, 1, 0, 2, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([2]), tensor([0])]\n",
            "0.713125\n",
            "***********\n",
            "tensor([2, 2, 0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 0, 2, 2], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([2]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0])]\n",
            "0.7122524752475248\n",
            "***********\n",
            "tensor([0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([2])]\n",
            "0.7132352941176471\n",
            "***********\n",
            "tensor([1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 1, 1], device='cuda:0')\n",
            "[tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([2]), tensor([2])]\n",
            "0.712378640776699\n",
            "***********\n",
            "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 2, 2, 0, 1, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])]\n",
            "0.7115384615384616\n",
            "***********\n",
            "tensor([0, 2, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1, 1, 2, 0, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([2]), tensor([0]), tensor([1]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([2]), tensor([0])]\n",
            "0.7119047619047619\n",
            "***********\n",
            "tensor([1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "[tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([1]), tensor([0]), tensor([0])]\n",
            "0.7110849056603774\n",
            "***********\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 1, 2, 2, 0, 2, 2, 2, 0, 2], device='cuda:0')\n",
            "[tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2])]\n",
            "0.709696261682243\n",
            "***********\n",
            "tensor([0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 2, 1, 0, 0], device='cuda:0')\n",
            "[tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([0]), tensor([1]), tensor([2]), tensor([0]), tensor([1]), tensor([2]), tensor([2]), tensor([1]), tensor([0]), tensor([0])]\n",
            "0.7094907407407407\n",
            "***********\n",
            "tensor([1, 1, 2, 0, 2, 2, 0, 2, 0, 0, 1, 1, 0, 2, 1, 1], device='cuda:0')\n",
            "[tensor([0]), tensor([1]), tensor([2]), tensor([1]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([2]), tensor([1]), tensor([0])]\n",
            "0.7092889908256881\n",
            "***********\n",
            "tensor([1, 2, 2, 0, 0, 1, 1, 1, 2, 0, 2, 0, 2, 2, 0, 1], device='cuda:0')\n",
            "[tensor([1]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([1])]\n",
            "0.709659090909091\n",
            "***********\n",
            "tensor([2, 2, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 2, 2], device='cuda:0')\n",
            "[tensor([2]), tensor([2]), tensor([1]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([0]), tensor([2]), tensor([2])]\n",
            "0.7105855855855856\n"
          ]
        }
      ],
      "execution_count": 72,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}